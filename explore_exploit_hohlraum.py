import pyapprox.multifidelity.etc as etc
from pyapprox.variables.joint import IndependentMarginalsVariable
from pyapprox.multifidelity.groupacv import get_model_subsets
import numpy as np
from scipy import stats
import time
import pickle

# import model from client_hohlraum
np.random.seed(1)

from src.models.hohlraum import get_qois_col_names, model
from src.general_utils import parse_args
from src.general_utils import (
    create_hohlraum_samples_from_param_range,
    load_hohlraum_samples_from_npz,
    delete_slurm_scripts,
)
from src.simulation_utils import execute_slurm_scripts, wait_for_slurm_jobs
from src.config_utils import read_username_from_config


np.random.seed(1)
# import model from client_hohlraum


def generate_model(cell_length, nquad, hpc_operation, singularity_hpc, qoi_idx):
    """
    Return a function that can be passed to the AETC algorithm
    cell_length (float): the model hyperparameter
    nquad (int): the model hyperparameter
    hpc_operation_count (int): parameter to indicate how the model is run
    singularity_hpc (bool): parameter to indicate how the model is run
    qoi_idx (int): Index of the qoi whose mean we want to estimate
    """

    # np.ndarray (nvars, nsamples) -> np.ndarary (nsamples, nqoi)
    def eval_model(samples):
        size = samples.shape
        extras = np.zeros((4, size[1]))
        extras[2, :] = cell_length
        extras[3, :] = nquad
        design_params = np.concatenate((samples, extras), axis=0)

        if hpc_operation:
            print("==== Execute HPC version ====")
            directory = "./benchmarks/hohlraum/slurm_scripts/"
            user = read_username_from_config("./slurm_config.txt")

            delete_slurm_scripts(directory)  # delete existing slurm files for hohlraum
            call_models(
                design_params, hpc_operation_count=1, singularity_hpc=singularity_hpc
            )
            wait_for_slurm_jobs(user=user, sleep_interval=10)
            if user:
                print("Executing slurm scripts with user " + user)
                execute_slurm_scripts(directory, user)
                wait_for_slurm_jobs(user=user, sleep_interval=10)
            else:
                print("Username could not be read from slurm config file.")
                exit(1)
            time.sleep(10)

            qois = call_models(design_params, hpc_operation_count=2)
        else:
            qois = call_models(design_params, hpc_operation_count=0)

        qois = np.array(qois)
        # wall_times = qois[0]
        qoi_result = qois[:, qoi_idx].reshape(-1, 1)
        return qoi_result

    def call_models(design_params, hpc_operation_count, singularity_hpc=True):
        qois = []
        for column in design_params.T:
            input = column.tolist()
            input.append(hpc_operation_count)
            input.append(singularity_hpc)
            res = model([input])
            qois.append(res[0])

        return np.array(qois)

    return eval_model


def main():

    args = parse_args()
    print(f"HPC mode = { not args.no_hpc}")
    print(f"Load from npz = {args.load_from_npz}")
    print(f"HPC with singularity = { not args.no_singularity_hpc}")

    hpc_operation = not args.no_hpc  # Flag when using HPC cluster
    singularity_hpc = not args.no_singularity_hpc

    # Define the model parameters that declare the fidelity
    #   Each element is one fidelity
    #   Highest fidelity comes first
    cell_lengths = np.linspace(7.5e-3, 5e-2, 9)
    nquads = 20 * np.ones(len(cell_lengths))

    # Generate the models into an array
    #   Include the correct HPC parameters
    models = np.array(
        [
            generate_model(
                cell_length=cl,
                nquad=nq,
                hpc_operation=hpc_operation,
                singularity_hpc=singularity_hpc,
                qoi_idx=1,
            )
            for (cl, nq) in zip(cell_lengths, nquads)
        ]
    )

    # Update the ranges of the variables to match what CharmKiT needs
    # marginals = [stats.uniform(0.1, 0.2)]*4 + [stats.uniform(0.025, 0.05)]*2
    marginals = [  # first entry: lower bound, second entry: length of intervall
        stats.uniform(0.3, 0.2),  # left red top
        stats.uniform(-0.5, 0.2),  # left red bottom
        stats.uniform(0.3, 0.2),  # right red top
        stats.uniform(-0.5, 0.2),  # right red bottom
        stats.uniform(-0.625, 0.05),  # left red width
        stats.uniform(0.575, 0.05),  # right red width
    ]
    variable = IndependentMarginalsVariable(marginals)
    sample_variable = variable.rvs

    # Run each model 1 time to estimate the model cost in seconds
    #   In the future, this will be done inside of the AETC algorithm to progressively get better estimates
    n_time_samples = 1
    costs = []
    for model in models:
        s = time.time()
        model(sample_variable(n_time_samples))
        e = time.time()
        costs.append((e - s) / n_time_samples)

    print("Costs: ", [f"{costs[i]:.2E}" for i, _ in enumerate(models)])

    # This is the total budget in seconds that the total model run time will not exceed
    #   Note that since the model costs are estimates, the algorithm may take longer than the given budget
    #   This budget does not include the overhead that the AETC takes to run
    budget = 345600

    # Construct the estimator
    optim_options = {"method": "cvxpy", "solver": "CLARABEL"}
    estimator = etc.AETCBLUE(
        models, sample_variable, costs=costs, opt_options=optim_options
    )

    # Explore phase
    lf_subsets = get_model_subsets(len(models) - 1, 4)
    samples, values, result = estimator.explore(budget, lf_subsets)
    result_dict = estimator._explore_result_to_dict(result)
    np.save("./hohlraum_explore_values.npy", values)
    np.save("./hohlraum_explore_result.npy", result_dict)

    # Exploit phase
    samples_per_model, best_subset = estimator.get_exploit_samples(result)

    # Run the `best_subset` models here using the input samples: `samples_per_model`
    values_per_model = [
        models[s](samples) for s, samples in zip(best_subset, samples_per_model)
    ]
    np.save("./hohlraum_exploit_values.npy", values_per_model)

    # Run the estimator
    mean = estimator.find_exploit_mean(values_per_model, result)

    # Print or save the results
    print("AETC Algorithm ran successfully")
    print("Mean of qoi:", mean)
    print("Result Dictionary:", result)


if __name__ == "__main__":
    main()
